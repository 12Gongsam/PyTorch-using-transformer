{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNpg3ZGmfILs09xxGuBnrGL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"7gwnOOLsF9ND","executionInfo":{"status":"ok","timestamp":1713133049504,"user_tz":-540,"elapsed":2,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}}},"outputs":[],"source":["import torch\n","from torch import nn"]},{"cell_type":"markdown","source":["# 배치 정규화"],"metadata":{"id":"l9KgqgHgLzuf"}},{"cell_type":"markdown","source":["- **배치 정규화(Batch Normalization)**란 **내부 공변량 변화(Internal Covariate Shift)**를 줄여 과대적합을 방지하는 기술이다.\n","- 각 계층은 배치 단위의 데이터로 인해 계속 변화되는 입력 분포를 학습해야 하기 때문에 인공 신경망의 성능과 안정성이 낮아져 학습 속도가 느려진다. 내부 공변량 변화란 이렇게 계층마다 입력 분포가 변경되는 현상을 의미한다."],"metadata":{"id":"zsWNpjx6mcz3"}},{"cell_type":"code","source":["x = torch.FloatTensor(\n","    [\n","        [-0.6577, -0.5797, 0.6360],\n","        [0.7392, 0.2145, 1.523],\n","        [0.2432, 0.5662, 0.322]\n","    ]\n",")"],"metadata":{"id":"9Qg1RmNdLpto","executionInfo":{"status":"ok","timestamp":1713132161220,"user_tz":-540,"elapsed":269,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["배치 정규화는 feature마다 따로 계산된다."],"metadata":{"id":"GIdz4uPbof6G"}},{"cell_type":"code","source":["print(nn.BatchNorm1d(num_features=3)(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HHFRRJyL1PX","executionInfo":{"status":"ok","timestamp":1713133053001,"user_tz":-540,"elapsed":3,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"c4578a85-b305-4df7-f774-80e1fbfe895a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1.3246, -1.3492, -0.3756],\n","        [ 1.0912,  0.3077,  1.3685],\n","        [ 0.2334,  1.0415, -0.9930]], grad_fn=<NativeBatchNormBackward0>)\n"]}]},{"cell_type":"markdown","source":["# 가중치 초기화"],"metadata":{"id":"91maAYuCMU2R"}},{"cell_type":"markdown","source":["- 상수 초기화: 모든 가중치를 매우 작은 양의 상수값으로 동일하게 할당 -> 대칭 파괴 현상으로 모델을 학습하기 어렵거나 불가능하게 만든다.\n","- 무작위 초기화: 초기 가중치를 무작위나 특정 분포 형태로 초기화 하는 것으로 random, uniform distribution, normal distribution, truncated normal distribution, sparse normal distribution 등이 있다."],"metadata":{"id":"nMAYkvqTpFzc"}},{"cell_type":"markdown","source":["- 자주 사용하는 초기화 방법으로 Xavier와 He가 있다. Xavier는 이전 계층과 다음 계층의 노드 수를 고려하는 방법으로 입력 데이터의 분산이 출력 데이터에서 유지되도록 가중치를 초기화해 시그모이드나 하이퍼볼릭 탄젠트를 활성화 함수로 사용하는 네트워크에서 효과적이다.  \n","- He는 현재 계층의 입력 뉴런수만 고려한 방법으로 각 노드의 출력 분산이 입력 분산과 동일하게 만들어 ReLU를 사용하는 네트워크에서 효과적이다."],"metadata":{"id":"Zs-4vP_oq-wH"}},{"cell_type":"markdown","source":["## 가중치 초기화 함수 (1)"],"metadata":{"id":"N2zUe6rKN8mP"}},{"cell_type":"code","source":["class Net(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer = nn.Sequential(\n","        nn.Linear(1, 2),\n","        nn.Sigmoid(),\n","    )\n","    self.fc = nn.Linear(2, 1)\n","    self._init_weights()\n","\n","  def _init_weights(self): # Protected Method\n","    nn.init.xavier_uniform_(self.layer[0].weight)\n","    self.layer[0].bias.data.fill_(0.01)\n","\n","    nn.init.xavier_uniform_(self.fc.weight)\n","    self.fc.bias.data.fill_(0.01)\n","\n","model = Net()"],"metadata":{"id":"Cz2KwvSaL958","executionInfo":{"status":"ok","timestamp":1713133183796,"user_tz":-540,"elapsed":296,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## 가중치 초기화 함수 (2)"],"metadata":{"id":"41CYrjVZPLmF"}},{"cell_type":"markdown","source":["모델이 커지고 구조가 복잡한 경우 초기화 메서드를 모듈화해 적용한다"],"metadata":{"id":"r3fLq5ERPNjM"}},{"cell_type":"code","source":["class Net(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer = nn.Sequential(\n","        nn.Linear(1, 2),\n","        nn.Sigmoid(),\n","    )\n","    self.fc = nn.Linear(2, 1)\n","    self.apply(self._init_weights)\n","\n","  def _init_weights(self, module):\n","    if isinstance(module, nn.Linear):\n","      nn.init.xavier_normal_(module.weight)\n","      nn.init.constant_(module.bias, 0.01)\n","      print(f\"Apply : {module}\")\n","\n","model = Net()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QU3ZYJo4PK4i","executionInfo":{"status":"ok","timestamp":1713133485250,"user_tz":-540,"elapsed":304,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"a94c3b8e-161e-43f8-8bc3-f78cb2048328"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Apply : Linear(in_features=1, out_features=2, bias=True)\n","Apply : Linear(in_features=2, out_features=1, bias=True)\n"]}]},{"cell_type":"markdown","source":["# 정칙화"],"metadata":{"id":"_-Avkml6QvNv"}},{"cell_type":"markdown","source":["## L1"],"metadata":{"id":"tWAUhS2rRgUn"}},{"cell_type":"code","source":["for x, y in train_dataloader:\n","  x = x.to(device)\n","  y = y.to(device)\n","\n","  output = model(x)\n","\n","  _lambda = 0.5\n","  l1_loss = sum(p.abs().sum() for p in model.parameters())\n","\n","  loss = criterion(output, y) + _lambda * l1_loss"],"metadata":{"id":"BY4hMCSEQg8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## L2"],"metadata":{"id":"Q9bkvZS2SMQU"}},{"cell_type":"code","source":["for x, y in train_dataloader:\n","  x = x.to(device)\n","  y = y.to(device)\n","\n","  output = model(x)\n","\n","  _lambda = 0.5\n","  l2_loss = sum(p.pow(2.0).sum() for p in model.parameters())\n","\n","  loss = criterion(output, y) + _lambda * l2_loss"],"metadata":{"id":"kHgPa7cLSNpd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 가중치 감쇠"],"metadata":{"id":"IiFVuePTT3vT"}},{"cell_type":"markdown","source":["파이토치에서 가중치 감쇠와 L2 정칙화는 동일하다"],"metadata":{"id":"abqIuStouHpe"}},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)"],"metadata":{"id":"O_oc5Y8HT80x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 드롭아웃"],"metadata":{"id":"E63EjnELURFu"}},{"cell_type":"code","source":["class Net(torch.nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer1 = torch.nn.Linear(10, 10)\n","    self.dropout = torch.nn.Dropout(p=0.5)\n","    self.layer2 = torch.nn.Linear(10, 10)\n","\n","  def forward(self, x):\n","    x = self.layer1(x)\n","    x = self.dropout(x)\n","    x = self.layer2(x)\n","    return x"],"metadata":{"id":"ji7gLMbUYta6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 그레디언트 클리핑"],"metadata":{"id":"Tv7lhbmgZb6e"}},{"cell_type":"code","source":["grad_norm = torch.nn.utils.clip_grad_norm_(\n","    parameters,\n","    max_norm,\n","    norm_type=2.0,\n",")"],"metadata":{"id":"heHH2XU9zLvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x, y in train_dataloader:\n","  x = x.to(device)\n","  y = y.to(device)\n","\n","  output = model(x)\n","  loss = criterion(output, y)\n","\n","  optimizer.zero_grad()\n","  loss.backward()\n","\n","  torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","\n","  optimizer.step()"],"metadata":{"id":"i4Px3RdeZcPs"},"execution_count":null,"outputs":[]}]}