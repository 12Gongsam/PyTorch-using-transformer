{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNB9wxmUAQAy5ef1wnFrVCJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install Korpora sentencepiece"],"metadata":{"id":"yYdaw2qq_W2_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 하위 단어 토큰화"],"metadata":{"id":"KoZavOaI90hr"}},{"cell_type":"markdown","source":["센텐스피스와 코포라 라이브러리르 통해 바이트 페어 인코딩을 수행하는 토크나이저 모델을 학습해 본다."],"metadata":{"id":"T90jzKGy_DJ4"}},{"cell_type":"markdown","source":["## 청와대 청원 데이터 다운로드"],"metadata":{"id":"l6h0JKCu_R-h"}},{"cell_type":"code","source":["from Korpora import Korpora\n","\n","corpus = Korpora.load(\"korean_petitions\")\n","dataset = corpus.train\n","petition = dataset[0]\n","\n","print(\"청원 시작일 :\", petition.begin)\n","print(\"청원 종료일 :\", petition.end)\n","print(\"청원 동의 수 :\", petition.num_agree)\n","print(\"청원 범주 :\", petition.category)\n","print(\"청원 제목 :\", petition.title)\n","print(\"청원 본문 :\", petition.text[:30])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODvytWHr9l0F","executionInfo":{"status":"ok","timestamp":1702820314977,"user_tz":-540,"elapsed":25304,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"f5d11249-69f4-4d25-b354-e4e81fd06277"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n","    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n","\n","    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n","    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n","    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n","\n","    # Description\n","    Author : Hyunjoong Kim lovit@github\n","    Repository : https://github.com/lovit/petitions_archive\n","    References :\n","\n","    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n","    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n","    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n","    단 청원의 동의 개수는 수집됩니다.\n","    자세한 내용은 위의 repository를 참고하세요.\n","\n","    # License\n","    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n","    Details in https://creativecommons.org/publicdomain/zero/1.0/\n","\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-08\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-09\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-10\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-11\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-12\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-01\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-02\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-03\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-04\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-05\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-06\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-07\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-08\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-09\n","[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-10\n"]},{"output_type":"stream","name":"stderr","text":["[korean_petitions] download petitions_2018-11: 37.7MB [00:00, 141MB/s]                            \n","[korean_petitions] download petitions_2018-12: 33.0MB [00:00, 121MB/s]                            \n","[korean_petitions] download petitions_2019-01: 34.8MB [00:00, 170MB/s]                            \n","[korean_petitions] download petitions_2019-02: 30.8MB [00:00, 156MB/s]                            \n","[korean_petitions] download petitions_2019-03: 34.9MB [00:00, 149MB/s]                            \n"]},{"output_type":"stream","name":"stdout","text":["청원 시작일 : 2017-08-25\n","청원 종료일 : 2017-09-24\n","청원 동의 수 : 88\n","청원 범주 : 육아/교육\n","청원 제목 : 학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.\n","청원 본문 : 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비\n"]}]},{"cell_type":"markdown","source":["## 학습 데이터세트 생성"],"metadata":{"id":"dt4AWpL3AO4I"}},{"cell_type":"code","source":["petitions = corpus.get_all_texts()\n","with open(\"../corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n","    for petition in petitions:\n","        f.write(petition + \"\\n\")"],"metadata":{"id":"sH5PNkS5AQen"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 토크나이저 모델 학습"],"metadata":{"id":"BcMRvt6wBKJM"}},{"cell_type":"code","source":["from sentencepiece import SentencePieceTrainer\n","\n","\n","SentencePieceTrainer.Train(\n","    \"--input=../corpus.txt\\\n","    --model_prefix=petition_bpe\\\n","    --vocab_size=8000 model_type=bpe\"\n",")"],"metadata":{"id":"SFupwQprBHHs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 바이트 페어 인코딩 토큰화"],"metadata":{"id":"s16cwQUEB9dE"}},{"cell_type":"code","source":["from sentencepiece import SentencePieceProcessor\n","\n","\n","tokenizer = SentencePieceProcessor()\n","tokenizer.load(\"petition_bpe.model\")\n","\n","sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n","sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n","\n","tokenized_sentence = tokenizer.encode_as_pieces(sentence)\n","tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n","print(\"단일 문장 토큰화 :\", tokenized_sentence)\n","print(\"여러 문장 토큰화 :\", tokenized_sentences)\n","\n","encoded_sentence = tokenizer.encode_as_ids(sentence)\n","encoded_sentences = tokenizer.encode_as_ids(sentences)\n","print(\"단일 문장 정수 인코딩 :\", encoded_sentence)\n","print(\"여러 문장 정수 인코딩 :\", encoded_sentences)\n","\n","decode_ids = tokenizer.decode_ids(encoded_sentences)\n","decode_pieces = tokenizer.decode_pieces(encoded_sentences)\n","print(\"정수 인코딩에서 문장 변환 :\", decode_ids)\n","print(\"하위 단어 토큰에서 문장 변환 :\", decode_pieces)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4kzoig0bB_Of","executionInfo":{"status":"ok","timestamp":1702821101035,"user_tz":-540,"elapsed":372,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"f1c02141-5ab1-45c6-c885-dd3de6e90710"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단일 문장 토큰화 : ['▁안녕하세요', ',', '▁토', '크', '나', '이', '저', '가', '▁잘', '▁학', '습', '되었', '군요', '!']\n","여러 문장 토큰화 : [['▁이렇게', '▁입', '력', '값을', '▁리', '스트', '로', '▁받아서'], ['▁쉽게', '▁토', '크', '나', '이', '저', '를', '▁사용할', '▁수', '▁있', '답니다']]\n","단일 문장 정수 인코딩 : [667, 6553, 994, 6880, 6544, 6513, 6590, 6523, 161, 110, 6554, 872, 787, 6648]\n","여러 문장 정수 인코딩 : [[372, 182, 6677, 4433, 1772, 1613, 6527, 4162], [1681, 994, 6880, 6544, 6513, 6590, 6536, 5852, 19, 5, 2639]]\n","정수 인코딩에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n","하위 단어 토큰에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n"]}]},{"cell_type":"markdown","source":["## 어휘 사전 불러오기"],"metadata":{"id":"PCrbozkvClRl"}},{"cell_type":"code","source":["tokenizer = SentencePieceProcessor()\n","tokenizer.load(\"petition_bpe.model\")\n","\n","vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n","print(list(vocab.items())[:5])\n","print(\"vocab size :\", len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86J07sl4Cm9e","executionInfo":{"status":"ok","timestamp":1702821103868,"user_tz":-540,"elapsed":390,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"8e3946fe-f15b-4993-d393-c55ac85c0f41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '니다'), (4, '▁이')]\n","vocab size : 8000\n"]}]},{"cell_type":"markdown","source":["워드 피스 : 빈도 기반이 아닌 확률 기반으로 글자 쌍을 병합"],"metadata":{"id":"cGeBVpPFC7aE"}},{"cell_type":"code","source":["!pip install tokenizers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n74Ou4RrDVAB","executionInfo":{"status":"ok","timestamp":1702821204035,"user_tz":-540,"elapsed":7476,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"b01dc2d4-f9b5-4705-b6fa-8e9445a6d810"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.19.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n"]}]},{"cell_type":"markdown","source":["## 워드피스 토크나이저 학습"],"metadata":{"id":"dXw_um_eDZuh"}},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.normalizers import Sequence, NFD, Lowercase\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","\n","tokenizer = Tokenizer(WordPiece())\n","# normalizers 모듈에 포함된 클래스를 불러와 시퀀스 형식으로 인스턴스를 전달\n","# NFD 유니코드 정규화, 소문자 변환을 사용\n","tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n","# pre_tokenizers 모듈에 포함된 클래스를 불러와 적용\n","# 공백과 구두점을 기준으로 분리\n","tokenizer.pre_tokenizer = Whitespace()\n","\n","tokenizer.train([\"../corpus.txt\"])\n","tokenizer.save(\"../petition_wordpiece.json\")"],"metadata":{"id":"Su8ZfaPOC9cO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 워드피스 토큰화"],"metadata":{"id":"a8hfpqDsEbX_"}},{"cell_type":"code","source":["from tokenizers.decoders import WordPiece as WordPieceDecoder\n","\n","\n","tokenizer = Tokenizer.from_file(\"../petition_wordpiece.json\")\n","tokenizer.decoder = WordPieceDecoder()\n","\n","sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n","sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n","\n","encoded_sentence = tokenizer.encode(sentence)\n","encoded_sentences = tokenizer.encode_batch(sentences)\n","\n","print(\"인코더 형식 :\", type(encoded_sentence))\n","\n","print(\"단일 문장 토큰화 :\", encoded_sentence.tokens)\n","print(\"여러 문장 토큰화 :\", [enc.tokens for enc in encoded_sentences])\n","\n","print(\"단일 문장 정수 인코딩 :\", encoded_sentence.ids)\n","print(\"여러 문장 정수 인코딩 :\", [enc.ids for enc in encoded_sentences])\n","\n","print(\"정수 인코딩에서 문장 변환 :\", tokenizer.decode(encoded_sentence.ids))"],"metadata":{"id":"mlFxWRmTEcjd"},"execution_count":null,"outputs":[]}]}