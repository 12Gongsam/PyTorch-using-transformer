{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyPI62wFKv5N2L1X0uqVytHR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install Korpora konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":541},"id":"5e-mHrXlHHaT","executionInfo":{"status":"ok","timestamp":1704785162608,"user_tz":-540,"elapsed":7680,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"c4611605-c4f4-46a0-dfb4-e2501eee10f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Korpora\n","  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m729.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dataclasses>=0.6 (from Korpora)\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (1.23.5)\n","Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (4.66.1)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.31.0)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.0.1)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2023.11.17)\n","Installing collected packages: dataclasses, JPype1, Korpora, konlpy\n","Successfully installed JPype1-1.5.0 Korpora-0.2.0 dataclasses-0.6 konlpy-0.6.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["# 순환 신경망"],"metadata":{"id":"eW3CYDNeB2pz"}},{"cell_type":"markdown","source":["## RNN"],"metadata":{"id":"1aMdEKh8E79C"}},{"cell_type":"markdown","source":["양방향 다층 신경망"],"metadata":{"id":"BNUh_N0-CNdh"}},{"cell_type":"markdown","source":["- 입력 차원 : [배치 크기, 시퀀스 길이, 입력 특성 크기]\n","- 초기 은닉 상태 : [계층 수 * 양방향 여부 + 1, 배치 크기, 은닉 상태 크기]"],"metadata":{"id":"LjKIiEnCDELf"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STF5T0oP8-wN","executionInfo":{"status":"ok","timestamp":1704784026297,"user_tz":-540,"elapsed":5798,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"8498ac32-4d8f-4470-d0e9-ad593daef704"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 6, 512])\n","torch.Size([6, 4, 256])\n"]}],"source":["import torch\n","from torch import nn\n","\n","input_size = 128\n","output_size = 256\n","num_layers = 3\n","bidirectional = True\n","\n","model = nn.RNN(\n","    input_size=input_size,\n","    hidden_size=output_size,\n","    num_layers=num_layers,\n","    nonlinearity='tanh',\n","    bidirectional=bidirectional,\n","    batch_first=True\n",")\n","\n","batch_size = 4\n","sequence_len = 6\n","\n","inputs = torch.randn(batch_size, sequence_len, input_size)\n","h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size) # 초기 은닉 상태\n","\n","outputs, hidden = model(inputs, h_0)\n","\n","print(outputs.shape)\n","print(hidden.shape)"]},{"cell_type":"markdown","source":["## LSTM"],"metadata":{"id":"31I_Ml_ME957"}},{"cell_type":"markdown","source":["양방향 다층 LSTM"],"metadata":{"id":"U-zU0qVPD67s"}},{"cell_type":"code","source":["input_size = 128\n","ouput_size = 256\n","num_layers = 3\n","bidirectional = True\n","proj_size = 64\n","\n","model = nn.LSTM(\n","    input_size=input_size,\n","    hidden_size=ouput_size,\n","    num_layers=num_layers,\n","    batch_first=True,\n","    bidirectional=bidirectional,\n","    proj_size=proj_size,\n",")\n","\n","batch_size = 4\n","sequence_len = 6\n","\n","inputs = torch.randn(batch_size, sequence_len, input_size)\n","h_0 = torch.rand(\n","    num_layers * (int(bidirectional) + 1),\n","    batch_size,\n","    proj_size if proj_size > 0 else ouput_size,\n",")\n","c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size)\n","\n","outputs, (h_n, c_n) = model(inputs, (h_0, c_0))\n","\n","print(outputs.shape)\n","print(h_n.shape)\n","print(c_n.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SY_RpmkvDRoG","executionInfo":{"status":"ok","timestamp":1704784352589,"user_tz":-540,"elapsed":495,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"4fd26c9e-5abd-416e-95ad-d018e4aca6d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 6, 128])\n","torch.Size([6, 4, 64])\n","torch.Size([6, 4, 256])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:879: UserWarning: LSTM with projections is not supported with oneDNN. Using default implementation. (Triggered internally at ../aten/src/ATen/native/RNN.cpp:1492.)\n","  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"]}]},{"cell_type":"markdown","source":["## 모델 실습"],"metadata":{"id":"zntn8FOvE_ay"}},{"cell_type":"markdown","source":["RNN과 LSTM을 활용해 문장 긍/부정 분류 모델을 학습"],"metadata":{"id":"fUuc_pqRFB2P"}},{"cell_type":"markdown","source":["문장 분류 모델"],"metadata":{"id":"-GzskPtiFGpV"}},{"cell_type":"code","source":["class SentenceClassifier(nn.Module):\n","  def __init__(self, n_vocab, hidden_dim, embedding_dim,\n","               n_layers, dropout=0.5, bidirectional=True, model_type='lstm'):\n","    super().__init__()\n","\n","    self.embedding = nn.Embedding(\n","        num_embeddings=n_vocab,\n","        embedding_dim=embedding_dim,\n","        padding_idx=0\n","    )\n","    if model_type == \"rnn\":\n","      self.model = nn.RNN(\n","          input_size=embedding_dim,\n","          hidden_size=hidden_dim,\n","          num_layers=n_layers,\n","          bidirectional=bidirectional,\n","          dropout=dropout,\n","          batch_first=True,\n","      )\n","    elif model_type == \"lstm\":\n","      self.model = nn.LSTM(\n","          input_size=embedding_dim,\n","          hidden_size=hidden_dim,\n","          num_layers=n_layers,\n","          bidirectional=bidirectional,\n","          dropout=dropout,\n","          batch_first=True,\n","      )\n","\n","    if bidirectional:\n","      self.classifier = nn.Linear(hidden_dim * 2, 1)\n","    else:\n","      self.classifier = nn.Linear(hidden_dim, 1)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, inputs):\n","    embeddings = self.embedding(inputs)\n","    output, _ = self.model(embeddings)\n","    last_output = output[:, -1, :]\n","    last_output = self.dropout(last_output)\n","    logits = self.classifier(last_output)\n","    return logits"],"metadata":{"id":"cvJq5pN6FBOe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터세트 불러오기"],"metadata":{"id":"FckFLH7VHCvH"}},{"cell_type":"code","source":["import pandas as pd\n","from Korpora import Korpora\n","\n","\n","corpus = Korpora.load(\"nsmc\")\n","corpus_df = pd.DataFrame(corpus.test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EqCF6_JHAxi","executionInfo":{"status":"ok","timestamp":1704785171519,"user_tz":-540,"elapsed":4195,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"32d230d3-c04e-41d2-9ba1-b7cad855a35e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n","    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n","\n","    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n","    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n","    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n","\n","    # Description\n","    Author : e9t@github\n","    Repository : https://github.com/e9t/nsmc\n","    References : www.lucypark.kr/docs/2015-pyconkr/#39\n","\n","    Naver sentiment movie corpus v1.0\n","    This is a movie review dataset in the Korean language.\n","    Reviews were scraped from Naver Movies.\n","\n","    The dataset construction is based on the method noted in\n","    [Large movie review dataset][^1] from Maas et al., 2011.\n","\n","    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n","\n","    # License\n","    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n","    Details in https://creativecommons.org/publicdomain/zero/1.0/\n","\n"]},{"output_type":"stream","name":"stderr","text":["[nsmc] download ratings_train.txt: 14.6MB [00:00, 72.8MB/s]                            \n","[nsmc] download ratings_test.txt: 4.90MB [00:00, 31.5MB/s]                           \n"]}]},{"cell_type":"code","source":["train = corpus_df.sample(frac=0.9, random_state=42)\n","test = corpus_df.drop(train.index)\n","\n","print(train.head(5).to_markdown())\n","print(\"Training Data Size :\", len(train))\n","print(\"Testing Data Size :\", len(test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lu_FbJ-KHWMI","executionInfo":{"status":"ok","timestamp":1704785193996,"user_tz":-540,"elapsed":347,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"1f102b84-8c25-4978-aef4-f02565b15a6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["|       | text                                                                                     |   label |\n","|------:|:-----------------------------------------------------------------------------------------|--------:|\n","| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n","|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n","|   199 | 신날 것 없는 애니.                                                                       |       0 |\n","| 12447 | 잔잔 격동                                                                                |       1 |\n","| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n","Training Data Size : 45000\n","Testing Data Size : 5000\n"]}]},{"cell_type":"markdown","source":["데이터 토큰화 및 단어 사전 구축"],"metadata":{"id":"-G926xfFHkzZ"}},{"cell_type":"code","source":["from konlpy.tag import Okt\n","from collections import Counter\n","\n","\n","def build_vocab(corpus, n_vocab, special_tokens):\n","    counter = Counter()\n","    for tokens in corpus:\n","        counter.update(tokens)\n","    vocab = special_tokens\n","    for token, count in counter.most_common(n_vocab):\n","        vocab.append(token)\n","    return vocab\n","\n","\n","tokenizer = Okt()\n","train_tokens = [tokenizer.morphs(review) for review in train.text]\n","test_tokens = [tokenizer.morphs(review) for review in test.text]\n","\n","vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n","token_to_id = {token: idx for idx, token in enumerate(vocab)}\n","id_to_token = {idx: token for idx, token in enumerate(vocab)}\n","\n","print(vocab[:10])\n","print(len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8EX5fJwHmue","executionInfo":{"status":"ok","timestamp":1704785378335,"user_tz":-540,"elapsed":125052,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"26d0f335-d10f-47ba-ade7-0904d9fb61c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n","5002\n"]}]},{"cell_type":"markdown","source":["정수 인코딩 및 패딩"],"metadata":{"id":"4NFkMPSuHy9a"}},{"cell_type":"code","source":["import numpy as np\n","\n","def pad_sequences(sequences, max_length, pad_value):\n","  result = list()\n","  for sequence in sequences:\n","    sequence = sequence[:max_length]\n","    pad_length = max_length - len(sequence)\n","    padded_sequence = sequence + [pad_value] * pad_length\n","    result.append(padded_sequence)\n","  return np.asarray(result)\n","\n","unk_id = token_to_id[\"<unk>\"]\n","train_ids = [\n","    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n","]\n","test_ids = [\n","    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n","]\n","\n","max_length = 32\n","pad_id = token_to_id[\"<pad>\"]\n","train_ids = pad_sequences(train_ids, max_length, pad_id)\n","test_ids = pad_sequences(test_ids, max_length, pad_id)\n","\n","print(train_ids[0])\n","print(test_ids[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWftLsMqHz-a","executionInfo":{"status":"ok","timestamp":1704785784941,"user_tz":-540,"elapsed":432,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"28f0f4f5-3454-4239-e040-85fbf0c27185"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n"," 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n","[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n","    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n"]}]},{"cell_type":"markdown","source":["데이터로더 적용"],"metadata":{"id":"1QrdTC9LJwN3"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","\n","train_ids = torch.tensor(train_ids)\n","test_ids = torch.tensor(test_ids)\n","\n","train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n","test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n","\n","train_dataset = TensorDataset(train_ids, train_labels)\n","test_dataset = TensorDataset(test_ids, test_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"],"metadata":{"id":"M6ejruunJxeF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["손실 함수와 최적화 함수 정의"],"metadata":{"id":"_UPXTZtiJ60Y"}},{"cell_type":"code","source":["from torch import optim\n","\n","\n","n_vocab = len(token_to_id)\n","hidden_dim = 64\n","embedding_dim = 128\n","n_layers = 2\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","classifier = SentenceClassifier(\n","    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",").to(device)\n","criterion = nn.BCEWithLogitsLoss().to(device)\n","optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"],"metadata":{"id":"fgoSBBbgJ8Oy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델 학습 및 테스트"],"metadata":{"id":"9VaTSzl0KT9A"}},{"cell_type":"code","source":["def train(model, datasets, criterion, optimizer, device, interval):\n","    model.train()\n","    losses = list()\n","\n","    for step, (input_ids, labels) in enumerate(datasets):\n","        input_ids = input_ids.to(device)\n","        labels = labels.to(device).unsqueeze(1)\n","\n","        logits = model(input_ids)\n","        loss = criterion(logits, labels)\n","        losses.append(loss.item())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if step % interval == 0:\n","            print(f\"Train Loss {step} : {np.mean(losses)}\")\n","\n","\n","def test(model, datasets, criterion, device):\n","    model.eval()\n","    losses = list()\n","    corrects = list()\n","\n","    for step, (input_ids, labels) in enumerate(datasets):\n","        input_ids = input_ids.to(device)\n","        labels = labels.to(device).unsqueeze(1)\n","\n","        logits = model(input_ids)\n","        loss = criterion(logits, labels)\n","        losses.append(loss.item())\n","        yhat = torch.sigmoid(logits)>.5\n","        corrects.extend(\n","            torch.eq(yhat, labels).cpu().tolist()\n","        )\n","\n","    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n","\n","\n","epochs = 5\n","interval = 500\n","\n","for epoch in range(epochs):\n","    train(classifier, train_loader, criterion, optimizer, device, interval)\n","    test(classifier, test_loader, criterion, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJVaNpAzJ_24","executionInfo":{"status":"ok","timestamp":1704786073301,"user_tz":-540,"elapsed":46535,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"e34fdbb0-2b5c-44d4-b945-e586872c36dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Loss 0 : 0.6867725849151611\n","Train Loss 500 : 0.6936247455859612\n","Train Loss 1000 : 0.6907391950086161\n","Train Loss 1500 : 0.682171862058684\n","Train Loss 2000 : 0.6720567100021853\n","Train Loss 2500 : 0.6646402294828337\n","Val Loss : 0.6168034291876772, Val Accuracy : 0.6782\n","Train Loss 0 : 0.5940654873847961\n","Train Loss 500 : 0.5844168995549817\n","Train Loss 1000 : 0.5647882052830288\n","Train Loss 1500 : 0.5538767893103105\n","Train Loss 2000 : 0.5439690747882413\n","Train Loss 2500 : 0.5374484636970922\n","Val Loss : 0.5012446052540606, Val Accuracy : 0.7622\n","Train Loss 0 : 0.554517388343811\n","Train Loss 500 : 0.5009422014632863\n","Train Loss 1000 : 0.4862860381513923\n","Train Loss 1500 : 0.4737898110847009\n","Train Loss 2000 : 0.46813520336377507\n","Train Loss 2500 : 0.4621158795046215\n","Val Loss : 0.44711719243861614, Val Accuracy : 0.8004\n","Train Loss 0 : 0.46445703506469727\n","Train Loss 500 : 0.410575522753055\n","Train Loss 1000 : 0.40422590316592394\n","Train Loss 1500 : 0.39952670239274457\n","Train Loss 2000 : 0.3960885977794205\n","Train Loss 2500 : 0.39488419776604394\n","Val Loss : 0.4035346594195777, Val Accuracy : 0.8172\n","Train Loss 0 : 0.40387678146362305\n","Train Loss 500 : 0.3576682861187977\n","Train Loss 1000 : 0.3543693164249996\n","Train Loss 1500 : 0.35621099595940964\n","Train Loss 2000 : 0.35406598514658044\n","Train Loss 2500 : 0.35390355664103185\n","Val Loss : 0.3949069145578927, Val Accuracy : 0.8198\n"]}]},{"cell_type":"markdown","source":["학습된 모델로부터 임베딩 추출"],"metadata":{"id":"OfW2JqQRK59T"}},{"cell_type":"code","source":["token_to_embedding = dict()\n","embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n","\n","for word, emb in zip(vocab, embedding_matrix):\n","  token_to_embedding[word] = emb\n","\n","token = vocab[1000]\n","print(token, token_to_embedding[token])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSryOJslK7YK","executionInfo":{"status":"ok","timestamp":1704786222913,"user_tz":-540,"elapsed":268,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"be952fdc-629a-4b60-c709-a7a2dc7b8bd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["보고싶다 [ 0.9156976  -0.55401576  1.8168696   1.2887877   0.02241418 -0.61677414\n","  0.09421384  1.2348244  -1.3410246   0.09005291  0.83459055  1.5599653\n","  0.09008738 -0.7722967  -1.0689512   0.40242195  0.5993975  -1.0171436\n","  0.76490223  1.1648107  -0.10444339  0.18582134  0.48415342  0.47307476\n"," -0.07117919  0.50694156 -0.02449991  0.8860589  -2.1261668   0.37267584\n","  1.395763   -0.22399338 -0.98970884  2.0817513   0.49492034  0.5672728\n","  0.80570793 -0.51452816 -0.2423634  -0.5295623  -2.767572   -0.60893476\n"," -0.50125694  1.2644466  -1.325586   -0.35699975  0.77019966  0.27758837\n","  1.6750653   0.88076276 -2.6860402   3.0295355   0.21739434  1.2575622\n"," -0.32314125 -0.18484998  0.9903218  -0.9300321  -0.75998497 -1.3938714\n"," -1.0250475  -0.4878564   1.892235    2.6940422  -0.02856987  0.06278264\n","  0.77608645 -0.6453776   0.01995367 -0.60925746 -0.9569977  -0.6779056\n","  1.0350417   0.9626615  -0.7517421   0.19573943 -1.3109112  -0.42941976\n","  0.5551294   1.6199611  -0.49342176  0.94042236  0.34073305  0.17974217\n"," -0.6445961   1.9881126  -1.9267832   0.48872843  0.8920549   0.5017308\n"," -1.432101    0.5430365  -1.6260115  -0.02909879  1.4827608   0.7210093\n"," -0.959514    1.3264253  -0.04167606 -0.32375333 -0.3421971  -0.5973011\n"," -1.0834634   0.19038841  0.06404561  0.5784112   1.4253327  -0.60482126\n"," -1.2246796   1.2418135  -1.0895737   0.6756441  -0.97556746 -0.52009875\n"," -0.95636415  0.41835323  0.6283442   0.5424502  -1.3228528  -0.11871856\n"," -0.3197381  -0.48346856 -2.1461508   0.8526704  -1.9427136   2.429411\n"," -1.5519657  -0.44550398]\n"]}]},{"cell_type":"markdown","source":["### 사전 학습된 모델로 임베딩 계층 초기화"],"metadata":{"id":"wpPMIAWqLs4J"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","tokens = [tokenizer.morphs(review) for review in corpus_df.text]\n","\n","word2vec = Word2Vec(\n","    sentences=tokens,\n","    vector_size=128,\n","    window=5,\n","    min_count=1,\n","    sg=1,\n","    epochs=3,\n","    max_final_vocab=10000\n",")"],"metadata":{"id":"ln2HeFjsLu1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_embedding = np.zeros((n_vocab, embedding_dim))\n","\n","for index, token in id_to_token.items():\n","  if token not in ['<pad>', '<unk>']:\n","    init_embedding[index] = word2vec.wv[token]\n","\n","embedding_layer = nn.Embedding.from_pretrained(\n","    torch.tensor(init_embedding, dtype=torch.float32)\n",")"],"metadata":{"id":"p01-V-itNzY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SentenceClassifier(nn.Module):\n","    def __init__(\n","        self,\n","        n_vocab,\n","        hidden_dim,\n","        embedding_dim,\n","        n_layers,\n","        dropout=0.5,\n","        bidirectional=True,\n","        model_type=\"lstm\",\n","        pretrained_embedding=None\n","    ):\n","        super().__init__()\n","        if pretrained_embedding is not None:\n","            self.embedding = nn.Embedding.from_pretrained(\n","                torch.tensor(pretrained_embedding, dtype=torch.float32)\n","            )\n","        else:\n","            self.embedding = nn.Embedding(\n","                num_embeddings=n_vocab,\n","                embedding_dim=embedding_dim,\n","                padding_idx=0\n","            )\n","\n","        if model_type == \"rnn\":\n","            self.model = nn.RNN(\n","                input_size=embedding_dim,\n","                hidden_size=hidden_dim,\n","                num_layers=n_layers,\n","                bidirectional=bidirectional,\n","                dropout=dropout,\n","                batch_first=True,\n","            )\n","        elif model_type == \"lstm\":\n","            self.model = nn.LSTM(\n","                input_size=embedding_dim,\n","                hidden_size=hidden_dim,\n","                num_layers=n_layers,\n","                bidirectional=bidirectional,\n","                dropout=dropout,\n","                batch_first=True,\n","            )\n","\n","        if bidirectional:\n","            self.classifier = nn.Linear(hidden_dim * 2, 1)\n","        else:\n","            self.classifier = nn.Linear(hidden_dim, 1)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, inputs):\n","        embeddings = self.embedding(inputs)\n","        output, _ = self.model(embeddings)\n","        last_output = output[:, -1, :]\n","        last_output = self.dropout(last_output)\n","        logits = self.classifier(last_output)\n","        return logits"],"metadata":{"id":"w1SZyqMsOXcb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier = SentenceClassifier(\n","    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n","n_layers=n_layers, pretrained_embedding=init_embedding\n",").to(device)\n","criterion = nn.BCEWithLogitsLoss().to(device)\n","optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n","\n","epochs = 5\n","interval = 500\n","\n","for epoch in range(epochs):\n","    train(classifier, train_loader, criterion, optimizer, device, interval)\n","    test(classifier, test_loader, criterion, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xbibGI3ObPg","executionInfo":{"status":"ok","timestamp":1704787123905,"user_tz":-540,"elapsed":47851,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"8a56f73c-0060-4a02-aae1-177665530a47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Loss 0 : 0.691292405128479\n","Train Loss 500 : 0.6692448214380565\n","Train Loss 1000 : 0.6627480921509502\n","Train Loss 1500 : 0.6475114267639602\n","Train Loss 2000 : 0.6250570234076969\n","Train Loss 2500 : 0.6136515822435846\n","Val Loss : 0.4809579539318054, Val Accuracy : 0.7774\n","Train Loss 0 : 0.47346651554107666\n","Train Loss 500 : 0.5059766391675153\n","Train Loss 1000 : 0.5005744232253714\n","Train Loss 1500 : 0.502079211249898\n","Train Loss 2000 : 0.49685681094115286\n","Train Loss 2500 : 0.4924142829052022\n","Val Loss : 0.46941806975835426, Val Accuracy : 0.7874\n","Train Loss 0 : 0.40222546458244324\n","Train Loss 500 : 0.4801936238230821\n","Train Loss 1000 : 0.4686541587307856\n","Train Loss 1500 : 0.46187163957411415\n","Train Loss 2000 : 0.4590513633190841\n","Train Loss 2500 : 0.4583448811126251\n","Val Loss : 0.426602516120996, Val Accuracy : 0.8016\n","Train Loss 0 : 0.666080892086029\n","Train Loss 500 : 0.43780473252375446\n","Train Loss 1000 : 0.4394672241631326\n","Train Loss 1500 : 0.4363539547006104\n","Train Loss 2000 : 0.4384947693806657\n","Train Loss 2500 : 0.4367974266344335\n","Val Loss : 0.4180565467372108, Val Accuracy : 0.804\n","Train Loss 0 : 0.5328929424285889\n","Train Loss 500 : 0.43229548597407197\n","Train Loss 1000 : 0.42501193516469976\n","Train Loss 1500 : 0.42134119163228223\n","Train Loss 2000 : 0.4193983819002452\n","Train Loss 2500 : 0.4205793669638706\n","Val Loss : 0.4091841897454125, Val Accuracy : 0.8092\n"]}]}]}