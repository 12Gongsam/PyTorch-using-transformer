{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyN6sbf2zGnmtEKGaLP/Xp0+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install Korpora konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":541},"id":"Ls61n_1GwC4b","executionInfo":{"status":"ok","timestamp":1704773378690,"user_tz":-540,"elapsed":8589,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"308d69bd-2bc8-4c47-ee01-c0b0986477dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Korpora\n","  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m880.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dataclasses>=0.6 (from Korpora)\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (1.23.5)\n","Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (4.66.1)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.31.0)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.0.1)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2023.11.17)\n","Installing collected packages: dataclasses, JPype1, Korpora, konlpy\n","Successfully installed JPype1-1.5.0 Korpora-0.2.0 dataclasses-0.6 konlpy-0.6.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["# N-gram"],"metadata":{"id":"qjxa44PPZ2pl"}},{"cell_type":"markdown","source":["N개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률을 추정한다.\n","<br><br>\n","다음은 간단하게 N-gram을 구현한 함수이다."],"metadata":{"id":"NS3eA2W4CbTv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmQpxU45UfjQ"},"outputs":[],"source":["def ngrams(sentence, n):\n","  words = sentence.split()\n","  ngrams = zip(*[words[i:] for i in range(n)])\n","  return list(ngrams)"]},{"cell_type":"code","source":["sentence = \"안녕하세요 만나서 진심으로 반가워요\"\n","\n","unigram = ngrams(sentence, 1)\n","bigram = ngrams(sentence, 2)\n","trigram = ngrams(sentence, 3)\n","\n","print(unigram)\n","print(bigram)\n","print(trigram)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mNh1fxmdaIp1","executionInfo":{"status":"ok","timestamp":1704757287115,"user_tz":-540,"elapsed":2,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"2d7972fa-23b6-47c7-ac0d-b82636134764"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n","[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n","[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"]}]},{"cell_type":"markdown","source":["또는 NLTK 라이브러리로 구현할 수 있다."],"metadata":{"id":"7ij-nR4xCwjb"}},{"cell_type":"code","source":["import nltk\n","\n","unigram = nltk.ngrams(sentence.split(), 1)\n","bigram = nltk.ngrams(sentence.split(), 2)\n","trigram = nltk.ngrams(sentence.split(), 3)\n","\n","print(list(unigram))\n","print(list(bigram))\n","print(list(trigram))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TqJIw48pacJn","executionInfo":{"status":"ok","timestamp":1704757289369,"user_tz":-540,"elapsed":1941,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"c9b8fb6a-8969-4f19-abc6-51851a24a393"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n","[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n","[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"]}]},{"cell_type":"markdown","source":["- 작은 규모의 데이터세트에서 연속된 문자열 패턴을 분석하는 데 큰 효과를 보인다.\n","- 예를 들어 '입이 무겁다'라는 표현 처럼 자주 등장하는 연속된 단어나 구를 추출하고, 이를 분석함으로써 관용적 표현을 파악할 수 있다.\n","- 단어의 순서가 중요한 자연어 처리 작업 및 문자열 패턴 분석에 활용된다."],"metadata":{"id":"7rioijtece31"}},{"cell_type":"markdown","source":["# TF-IDF"],"metadata":{"id":"a9qbe444c_Fr"}},{"cell_type":"markdown","source":["**TF-IDF(Term Frequency-Inverse Document Frequency)**란 텍스트에서 특정 단어의 중요도를 계산하는 방법으로, 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적인 가중치를 의미한다. 즉, **BoW(Bag of Words)**에 가중치를 부여하는 방법이다.  \n","<br>\n","BoW는 문서나 문장을 단어의 집합으로 표현하는 방법으로, 단어의 중복을 허용해 빈도를 기록한다."],"metadata":{"id":"dpNrPCZvC8x1"}},{"cell_type":"markdown","source":["- TF: 문서 내에서 특정 단어의 빈도수\n","- DF: 한 단어가 얼마나 많은 문서에 나타나는지 의미\n","- IDF: 전체 문서수를 DF로 나눈 뒤 로그를 취한 값이다. 즉, 특정 단어의 등장 횟수가 적으면 IDF 값은 상대적으로 커진다."],"metadata":{"id":"_-jfCqj0D_BB"}},{"cell_type":"markdown","source":["TF-IDF는 TF와 IDF를 곱한 값이다. 어떤 단어가 문서 내에서 자주 등장하지만 전체 문서 내에서는 적게 등장한다면 TF-IDF 값은 커진다. 그러므로 자주 등할 확률이 높은 관사나 관용어 등의 가중치는 낮아진다.\n","<br><br>\n","사이킷런을 활용해 TF-IDF를 계산할 수 있다."],"metadata":{"id":"OE7mI8yAEgdc"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","corpus = [\n","    \"That movie is famous movie\",\n","    \"I like that actor\",\n","    \"I don’t like that actor\"\n","]\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_vectorizer.fit(corpus)\n","tfidf_matrix = tfidf_vectorizer.transform(corpus)\n","# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n","\n","print(tfidf_matrix.toarray())\n","print(tfidf_vectorizer.vocabulary_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2lzjjJDcuwH","executionInfo":{"status":"ok","timestamp":1704758017625,"user_tz":-540,"elapsed":319,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"d5cb3e84-30a8-4118-aa70-cd4d5299424d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n","  0.2344005 ]\n"," [0.61980538 0.         0.         0.         0.61980538 0.\n","  0.48133417]\n"," [0.4804584  0.63174505 0.         0.         0.4804584  0.\n","  0.37311881]]\n","{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"]}]},{"cell_type":"markdown","source":["- 문서마다 중요한 단어만 추출할 수 있으며, 벡터값을 활용해 문서 내 핵심 단어를 추출할 수 있다.\n","- 빈도기반 덱터화는 문자으이 순서나 문맥을 고려하지 않는다. 그러므로 문장 생성과 같이 순서가 중요한 작업에는 부적합하다.\n","- 벡터가 단어의 의미를 담고 있지는 않다."],"metadata":{"id":"-fX8gph5gfVF"}},{"cell_type":"markdown","source":["# Word2Vec"],"metadata":{"id":"5IwDi8NNg-6Q"}},{"cell_type":"markdown","source":["Word2Vec은 임베딩 모델로 단어 간의 유사성을 측정하기 위해 **분포 가설**을 기반으로 개발됐다.\n","<br><br>\n","분포 가설이란 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정이다. 분포 가설은 ㄷ나어 간의 동시 발생 확률 분포를 이용해 단어 간의 유사성을 측정한다."],"metadata":{"id":"4fQvYAGrFH_x"}},{"cell_type":"markdown","source":["## Skip-gram"],"metadata":{"id":"dKlQuDH5vqT3"}},{"cell_type":"markdown","source":["Skip-gram은 중심 단어를 입력으로 받아서 주변 단어를 예측하는 모델이다.\n","<br><br>\n","학습 데이터가 어떻게 만들어지는지 알아보기 위해 \"A B C D E\"라는 문장이 있고 window를 2로 설정한다고 가정한다. 처음의 중심 단어는 A이고 주변 단어는 B, C이다. 학습 데이터는 (A | B), (A | C)이다. 중심 단어가 C일 때는 학습 데이터가 4개 만들어진다.\n","<br><br>\n","CBoW와 비교하여 여러 학습 데이터를 추출할 수 있어 더 높은 성능을 보인다. 계층적 소프트맥스나 네거티브 샘플링을 사용하지 않는 기본 Skip-gram 클래스는 다음과 같이 정의할 수 있다."],"metadata":{"id":"AHsBeMuqusPk"}},{"cell_type":"code","source":["from torch import nn\n","\n","class VanillaSkipGram(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super().__init__()\n","    self.embedding = nn.Embedding(\n","        num_embeddings=vocab_size,\n","        embedding_dim=embedding_dim\n","    )\n","    self.linear = nn.Linear(\n","        in_features=embedding_dim,\n","        out_features=vocab_size\n","    )\n","\n","  def forward(self, input_ids):\n","    embeddings = self.embedding(input_ids)\n","    output = self.linear(embeddings)\n","    return output"],"metadata":{"id":"tt1NC2njgv-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["영화 리뷰 데이터세트 전처리"],"metadata":{"id":"QCrmhr1BvvQ6"}},{"cell_type":"code","source":["import pandas as pd\n","from Korpora import Korpora\n","from konlpy.tag import Okt\n","\n","corpus = Korpora.load('nsmc')\n","corpus = pd.DataFrame(corpus.test)\n","\n","tokenizer = Okt()\n","tokens = [tokenizer.morphs(review) for review in corpus.text]\n","print(tokens[:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG5lqb4-vxAh","executionInfo":{"status":"ok","timestamp":1704773553101,"user_tz":-540,"elapsed":170700,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"187b791e-0e5e-4366-d1ae-92fa48183151"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n","    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n","\n","    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n","    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n","    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n","\n","    # Description\n","    Author : e9t@github\n","    Repository : https://github.com/e9t/nsmc\n","    References : www.lucypark.kr/docs/2015-pyconkr/#39\n","\n","    Naver sentiment movie corpus v1.0\n","    This is a movie review dataset in the Korean language.\n","    Reviews were scraped from Naver Movies.\n","\n","    The dataset construction is based on the method noted in\n","    [Large movie review dataset][^1] from Maas et al., 2011.\n","\n","    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n","\n","    # License\n","    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n","    Details in https://creativecommons.org/publicdomain/zero/1.0/\n","\n"]},{"output_type":"stream","name":"stderr","text":["[nsmc] download ratings_train.txt: 14.6MB [00:00, 241MB/s]\n","[nsmc] download ratings_test.txt: 4.90MB [00:00, 141MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"]}]},{"cell_type":"markdown","source":["단어 사전 구축"],"metadata":{"id":"Vy6Ia4vFzs6d"}},{"cell_type":"code","source":["from collections import Counter\n","\n","def build_vocab(corpus, n_vocab, special_tokens):\n","  counter = Counter()\n","  for tokens in corpus:\n","    counter.update(tokens)\n","  vocab = special_tokens\n","  for token, count in counter.most_common(n_vocab):\n","    vocab.append(token)\n","  return vocab\n","\n","vocab = build_vocab(tokens, 5000, ['<unk>'])\n","token_to_id = {token: idx for idx, token in enumerate(vocab)}\n","id_to_token = {idx: token for idx, token in enumerate(vocab)}\n","\n","print(vocab[:10])\n","print(len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7A20OHQzt9a","executionInfo":{"status":"ok","timestamp":1704773553101,"user_tz":-540,"elapsed":11,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"8d42cc3a-1ebd-434b-d6f1-ec9dc5f84157"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n","5001\n"]}]},{"cell_type":"markdown","source":["Skip-gram의 단어 쌍 추출"],"metadata":{"id":"mEV7KPG70rgl"}},{"cell_type":"code","source":["def get_word_pairs(tokens, window_size):\n","  pairs = []\n","  for sentence in tokens:\n","    sentence_length = len(sentence)\n","    for idx, center_word in enumerate(sentence):\n","      window_start = max(0, idx - window_size)\n","      window_end = min(sentence_length, idx + window_size + 1)\n","      center_word = sentence[idx]\n","      context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n","      for context_word in context_words:\n","        pairs.append([center_word, context_word])\n","  return pairs\n","\n","word_pairs = get_word_pairs(tokens, window_size=2)\n","print(word_pairs[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZgD4WSCP0tC-","executionInfo":{"status":"ok","timestamp":1704773555588,"user_tz":-540,"elapsed":2489,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"10dd1901-5a3d-45a8-9f49-357c95d871e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]\n"]}]},{"cell_type":"markdown","source":["단어 쌍을 인덱스 쌍으로 변환"],"metadata":{"id":"ZB1AnTnc1mDi"}},{"cell_type":"code","source":["def get_index_pairs(word_pairs, token_to_id):\n","  pairs = []\n","  unk_index = token_to_id['<unk>']\n","  for word_pair in word_pairs:\n","    centor_word, context_word = word_pair\n","    centor_word_index = token_to_id.get(centor_word, unk_index)\n","    context_word_index = token_to_id.get(context_word, unk_index)\n","    pairs.append([centor_word_index, context_word_index])\n","  return pairs\n","\n","index_pairs = get_index_pairs(word_pairs, token_to_id)\n","print(index_pairs[:5])\n","print(len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plP6Lhmi1oOM","executionInfo":{"status":"ok","timestamp":1704773558196,"user_tz":-540,"elapsed":2609,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"1706ae89-a810-4e53-8105-ce0ef30c24c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n","5001\n"]}]},{"cell_type":"markdown","source":["데이터 로더 적용"],"metadata":{"id":"ILbcdyYF2E6h"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","index_pairs = torch.tensor(index_pairs)\n","center_indexs = index_pairs[:, 0]\n","context_indexs = index_pairs[:, 1]\n","\n","dataset = TensorDataset(center_indexs, context_indexs)\n","dataloader = DataLoader(dataset, batch_size=256, shuffle=True)"],"metadata":{"id":"hxRk-_482GLw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Skip-gram 모델 준비 작업"],"metadata":{"id":"tEQPiT9c2b-n"}},{"cell_type":"code","source":["from torch import optim\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","word2vec = VanillaSkipGram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"],"metadata":{"id":"HT8OYZ322Zeo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델 학습"],"metadata":{"id":"-eCw-w4F2tjn"}},{"cell_type":"code","source":["for epoch in range(20):\n","    cost = 0.0\n","    for input_ids, target_ids in dataloader:\n","        input_ids = input_ids.to(device)\n","        target_ids = target_ids.to(device)\n","\n","        logits = word2vec(input_ids)\n","        loss = criterion(logits, target_ids)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        cost += loss\n","\n","    cost = cost / len(dataloader)\n","    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKNGodt62ubl","executionInfo":{"status":"ok","timestamp":1704774421073,"user_tz":-540,"elapsed":858589,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"fbd68dd9-c1eb-4dd3-aac9-8ed258c7d0ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch :    1, Cost : 6.618\n","Epoch :    2, Cost : 6.191\n","Epoch :    3, Cost : 6.090\n","Epoch :    4, Cost : 6.035\n","Epoch :    5, Cost : 6.000\n","Epoch :    6, Cost : 5.974\n","Epoch :    7, Cost : 5.954\n","Epoch :    8, Cost : 5.938\n","Epoch :    9, Cost : 5.925\n","Epoch :   10, Cost : 5.914\n","Epoch :   11, Cost : 5.904\n","Epoch :   12, Cost : 5.895\n","Epoch :   13, Cost : 5.887\n","Epoch :   14, Cost : 5.880\n","Epoch :   15, Cost : 5.874\n","Epoch :   16, Cost : 5.868\n","Epoch :   17, Cost : 5.862\n","Epoch :   18, Cost : 5.857\n","Epoch :   19, Cost : 5.853\n","Epoch :   20, Cost : 5.848\n"]}]},{"cell_type":"markdown","source":["임베딩 값 추출"],"metadata":{"id":"_SBFRcVb28GQ"}},{"cell_type":"code","source":["token_to_embedding = dict()\n","embedding_matrix = word2vec.embedding.weight.data.cpu().numpy()\n","\n","for word, embedding in zip(vocab, embedding_matrix):\n","  token_to_embedding[word] = embedding\n","\n","index = 30\n","token = vocab[30]\n","token_embedding = token_to_embedding[token]\n","print(token)\n","print(token_embedding)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvUZ-nsT29Gu","executionInfo":{"status":"ok","timestamp":1704774456173,"user_tz":-540,"elapsed":468,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"5ae1fe33-114a-4e5c-bf5e-6a36cd8fe6ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["연기\n","[ 0.45873535 -1.4125738  -1.3788592   0.7086098   1.2213398  -0.26372698\n"," -0.28037128 -1.4234469   1.5022029   1.2094473  -1.0158551   0.18843827\n","  0.50370085 -2.3361244  -0.06898285  0.9282817  -0.48920494  2.3391287\n","  1.9510529   1.0244179  -1.0589218   0.45513138 -0.26152292 -0.92398655\n"," -0.8618328  -0.23008227 -0.38347512  0.5492202   0.11639255  0.26918137\n","  1.3105102   0.93400407  0.02157224 -0.38650048 -1.4645252  -1.9102594\n","  0.7985533  -0.5641499  -0.67817944 -1.8232611   0.39260313  0.1452537\n"," -1.4572662   0.8155799   0.68955433  2.3247154  -2.3831      0.07159209\n","  1.9544561  -1.3041784   1.6265767  -0.47780564 -0.13338266 -1.2765781\n","  1.2211463  -0.54078066 -1.0523287  -0.11295447 -0.63868177 -0.6317604\n","  0.05026022 -1.5645266  -1.7714697   0.38536918  0.1453347   0.82799226\n","  0.5879392   1.2119329  -1.1189276  -1.6172574  -1.4835991  -1.2073246\n","  0.1977502   0.9653218  -0.16744262  1.2672088  -1.0045105   0.83066016\n","  0.93309015 -1.4094567  -0.04989136  1.165612   -2.3469994   0.48065534\n","  0.4089467   0.9083836   1.4834619   0.05953585  0.63869303 -1.077794\n"," -0.02494393 -0.6638253  -1.1920466  -0.16212064 -1.0870461   0.01633668\n","  0.8580365  -0.70808744 -0.69874567 -0.43113938 -1.0387203  -1.9231895\n"," -0.17656337 -0.4171057  -0.04092513 -1.8552705   0.8653499   0.6147957\n","  0.35898766  0.79467636  0.8459003   0.70341015 -0.08004533 -1.2992288\n"," -0.6225215   0.6057202   0.6025012   0.79180825 -0.32311577  0.7279106\n"," -0.99680936 -0.80647945 -0.31687465 -1.1623919   1.0356776   0.44287938\n"," -0.3521926   0.40414435]\n"]}]},{"cell_type":"markdown","source":["단어 임베딩 코사인 유사도 계산"],"metadata":{"id":"Mi6ehk-q3h2J"}},{"cell_type":"code","source":["import numpy as np\n","from numpy.linalg import norm\n","\n","def cosine_similarity(a, b):\n","  # a : token_embedding\n","  # b : embedding_matrix\n","  cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n","  return cosine\n","\n","def top_n_index(cosine_matrix, n):\n","  closest_indexes = cosine_matrix.argsort()[::-1]\n","  top_n = closest_indexes[1:n+1]\n","  return top_n"],"metadata":{"id":"afVDGgzD3jay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n","top_n = top_n_index(cosine_matrix, n=5)\n","\n","print(f\"{token}와 가장 유사한 5 개 단어\")\n","for index in top_n:\n","    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0qsWszJ6OW9","executionInfo":{"status":"ok","timestamp":1704774460037,"user_tz":-540,"elapsed":1,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"a7f66be9-cf6d-4dbc-ca88-40e4ca246db4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["연기와 가장 유사한 5 개 단어\n","처리 - 유사도 : 0.3358\n","어쩌면 - 유사도 : 0.3080\n","인들 - 유사도 : 0.3048\n","해야 - 유사도 : 0.2952\n","찬사 - 유사도 : 0.2937\n"]}]},{"cell_type":"markdown","source":["## Gensim"],"metadata":{"id":"clgksFJqa9GZ"}},{"cell_type":"markdown","source":["Gensim 라이브러리는 대용량 텍스트 데이터의 처리를 위한 메모리 효륭적인 방법을 제공해 대규모 데이터셋에서도 효과적으로 모델을 학습할 수 있다. 또한 학습된 모델을 저장하여 관리할 수 있고, 단어 유사도 등 관련된 기능도 제공한다.\n","<br><br>\n","다음은 Gensim을 Word2Vec 모델을 학습하는 예시이다."],"metadata":{"id":"mEX3uFY0IFH0"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","word2vec = Word2Vec(\n","    sentences=tokens,\n","    vector_size=128,\n","    window=5,\n","    min_count=1,\n","    sg=1,\n","    epochs=3,\n","    max_final_vocab=10000\n",")\n","\n","# word2vec.save(\"../models/word2vec.model\")\n","# word2vec = Word2Vec.load(\"../models/word2vec.model\")"],"metadata":{"id":"kvc2OO-ra-YE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word = \"연기\"\n","print(word2vec.wv[word])\n","print(word2vec.wv.most_similar(word, topn=5))\n","print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UirehtilcVOR","executionInfo":{"status":"ok","timestamp":1704774508030,"user_tz":-540,"elapsed":469,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"496957b7-fe33-45c8-aa0e-74761025ea03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-4.93290693e-01 -2.84351289e-01  3.69010538e-01  3.36962819e-01\n"," -9.64935124e-02 -5.49013987e-02 -4.85488214e-02 -1.27716467e-01\n"," -4.83507395e-01  3.21379542e-01  3.13959308e-02 -6.15233421e-01\n"," -2.92426646e-01  1.64555550e-01  3.66918594e-02 -8.56545269e-02\n"," -4.61444676e-01  3.18951048e-02 -1.03940899e-02  2.39758268e-01\n","  6.29726827e-01  4.55769122e-01 -1.13298431e-01 -9.43350568e-02\n"," -1.88041314e-01 -1.82925742e-02 -3.41772169e-01  1.44763231e-01\n","  1.03929475e-01 -1.62047133e-01 -3.31691086e-01 -6.38810918e-02\n","  2.35159218e-01 -1.64621472e-01  8.18434730e-02 -3.45117805e-05\n","  1.10119991e-02 -1.04571640e-01 -2.07111433e-01 -3.48203599e-01\n","  1.55132841e-02 -9.32942927e-02 -3.38449746e-01 -5.04595220e-01\n"," -2.27043867e-01  3.99443597e-01 -2.41629705e-01 -1.28127426e-01\n","  3.01755011e-01  8.97378400e-02  4.04503345e-01  2.81150848e-01\n","  2.06287652e-01  2.97228843e-01 -1.80032998e-01 -2.71678925e-01\n"," -2.24731162e-01  2.29277089e-01 -1.68268502e-01  2.22951025e-01\n","  5.27729876e-02 -1.21088989e-01  1.03757724e-01  3.59736569e-02\n"," -3.91371757e-01  9.36562847e-03  3.10790911e-02  2.94032753e-01\n","  3.91662449e-01 -3.29003572e-01 -4.48686063e-01 -3.47319096e-01\n"," -4.01768833e-01  6.67317659e-02 -7.32762888e-02 -2.12015480e-01\n"," -3.20789188e-01 -3.53328288e-01 -1.83699280e-01  9.93791595e-02\n"," -2.54023015e-01  3.87015976e-02  3.45913082e-01  7.76938021e-01\n","  4.97773200e-01  2.96262372e-02  3.85883182e-01 -4.51796770e-01\n","  2.37966701e-01  2.80202050e-02 -3.11047733e-01  1.96062271e-02\n","  2.97472239e-01  6.21288419e-01  3.94716188e-02  9.89301726e-02\n"," -2.22948164e-01 -5.74286468e-02 -2.38115683e-01 -6.46874070e-01\n"," -3.92658383e-01  1.74469166e-02  2.22188652e-01 -2.91427612e-01\n","  2.04873830e-01  4.87469971e-01  2.48682186e-01  1.73744127e-01\n","  5.57374395e-02 -3.58898371e-01  4.84102249e-01 -2.94970065e-01\n"," -1.65744528e-01  3.76391351e-01 -4.85834405e-02 -3.33001852e-01\n","  2.99326003e-01  1.78990975e-01 -9.90194976e-02  4.77505296e-01\n"," -3.92675884e-02 -1.67990878e-01  1.05300441e-01  5.53033017e-02\n","  2.69727204e-02 -2.42120504e-01 -4.65785593e-01 -8.79928917e-02]\n","[('연기력', 0.799852728843689), ('캐스팅', 0.7382062077522278), ('조연', 0.7178145051002502), ('목소리', 0.7166693806648254), ('연기자', 0.7116499543190002)]\n","0.79985267\n"]}]},{"cell_type":"markdown","source":["# fastText"],"metadata":{"id":"x2Oe8zJBcyED"}},{"cell_type":"markdown","source":["fastText 모델도 CBow와 Skip-Gram으로 구성된 임베딩 모델이다. Word2Vec과의 차이점은 Word2Vec은 단어를 기본 단위로 학습하지만, FastText는 하위 단어 집합으로 학습한다."],"metadata":{"id":"aANPyi7yIfSm"}},{"cell_type":"code","source":["corpus = Korpora.load(\"kornli\")\n","corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n","tokens = [sentence.split() for sentence in corpus_texts]\n","\n","print(tokens[:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gI_rI4mczv6","executionInfo":{"status":"ok","timestamp":1704774547120,"user_tz":-540,"elapsed":31368,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"b6459254-d444-4e6a-be4e-386976fff052"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n","    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n","\n","    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n","    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n","    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n","\n","    # Description\n","    Author : KakaoBrain\n","    Repository : https://github.com/kakaobrain/KorNLUDatasets\n","    References :\n","        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n","           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n","           (https://arxiv.org/abs/2004.03289)\n","\n","    This is the dataset repository for our paper\n","    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n","    (https://arxiv.org/abs/2004.03289)\n","    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n","\n","    # License\n","    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n","    Details in https://creativecommons.org/licenses/by-sa/4.0/\n","\n"]},{"output_type":"stream","name":"stderr","text":["[kornli] download multinli.train.ko.tsv: 83.6MB [00:00, 304MB/s]                            \n","[kornli] download snli_1.0_train.ko.tsv: 78.5MB [00:00, 184MB/s]                            \n","[kornli] download xnli.dev.ko.tsv: 516kB [00:00, 11.6MB/s]\n","[kornli] download xnli.test.ko.tsv: 1.04MB [00:00, 23.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[['개념적으로', '크림', '스키밍은', '제품과', '지리라는', '두', '가지', '기본', '차원을', '가지고', '있다.'], ['시즌', '중에', '알고', '있는', '거', '알아?', '네', '레벨에서', '다음', '레벨로', '잃어버리는', '거야', '브레이브스가', '모팀을', '떠올리기로', '결정하면', '브레이브스가', '트리플', 'A에서', '한', '남자를', '떠올리기로', '결정하면', '더블', 'A가', '그를', '대신하러', '올라가고', 'A', '한', '명이', '그를', '대신하러', '올라간다.'], ['우리', '번호', '중', '하나가', '당신의', '지시를', '세밀하게', '수행할', '것이다.']]\n"]}]},{"cell_type":"markdown","source":["fastText 모델 학습"],"metadata":{"id":"VxmD_CYofti_"}},{"cell_type":"code","source":["from gensim.models import FastText\n","\n","\n","fastText = FastText(\n","    sentences=tokens,\n","    vector_size=128,\n","    window=5,\n","    min_count=5,\n","    sg=1,\n","    epochs=3,\n","    min_n=2,\n","    max_n=6\n",")\n","\n","# fastText.save(\"../models/fastText.model\")\n","# fastText = FastText.load(\"../models/fastText.model\")"],"metadata":{"id":"dgI4JeJIerfr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["fastText OOV 처리"],"metadata":{"id":"C7tJzbyofqnt"}},{"cell_type":"code","source":["oov_token = \"사랑해요\"\n","oov_vector = fastText.wv[oov_token]\n","\n","print(oov_token in fastText.wv.index_to_key)\n","print(fastText.wv.most_similar(oov_vector, topn=5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHdNUaAtfGpr","executionInfo":{"status":"ok","timestamp":1704775065764,"user_tz":-540,"elapsed":484,"user":{"displayName":"JUNHA HWANG","userId":"15713937348463840886"}},"outputId":"8e06e68d-7ea2-4abf-b1c0-f5062f0376f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","[('사랑해', 0.9106492400169373), ('사랑', 0.8731779456138611), ('사랑한', 0.8662685751914978), ('사랑해서', 0.8453728556632996), ('사랑해.', 0.8408307433128357)]\n"]}]}]}